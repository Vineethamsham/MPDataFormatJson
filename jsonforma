def load_local_active_plans(path: str) -> List[Document]:
    documents = []
    idx = 0  # global chunk index

    for filename in os.listdir(path)[:10]:
        if not filename.endswith(".json"):
            continue

        with open(os.path.join(path, filename), "r", encoding="utf-8") as f:
            json_data = json.load(f)

        sections = json_data.get("sections", [])
        all_texts = []

        for section in sections:
            section_text_list = section.get("content", [])

            if isinstance(section_text_list, list):
                section_text = "\n".join(str(item) for item in section_text_list if isinstance(item, str))
            elif isinstance(section_text_list, str):
                section_text = section_text_list
            else:
                continue

            if section_text.strip():
                all_texts.append(section_text.strip())

        full_text = "\n\n".join(all_texts)

        if full_text.strip():
            doc = Document(
                page_content=full_text,
                metadata={
                    "plan_id": filename.split(".")[0],
                    "source_url": json_data.get("url", ""),
                    "version": "v1",
                    "category": "plans",
                }
            )

            # ✅ Use your old chunker
            token_len = len(full_text.split())  # or use tiktoken if configured
            if token_len > 8000:
                chunks = token_safe_split(doc, chunk_size=8000, chunk_overlap=250)
            else:
                chunks = [doc]

            # ✅ Assign chunk_index per document
            for i, chunk in enumerate(chunks):
                chunk.metadata["chunk_index"] = i
                documents.append(chunk)

            idx += 1

    return documents
